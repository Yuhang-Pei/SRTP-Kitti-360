{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read training data from pca_matrix.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entire pca_matrix\n",
    "def ReadPCAMatrixData(pca_matrix: list):\n",
    "    with open(file = 'pca_matrix.dat', mode = 'r') as pca_matrix_file:\n",
    "        while 1:\n",
    "            line = pca_matrix_file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line = [ord(x) - ord('0') for x in line]  # convert characters to integers\n",
    "            line.pop()\n",
    "            pca_matrix.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training data:  150\n",
      "dimension:  32768\n"
     ]
    }
   ],
   "source": [
    "pca_train_data = list()  # create an empty list\n",
    "ReadPCAMatrixData(pca_train_data)\n",
    "print('size of training data: ', len(pca_train_data))\n",
    "print('dimension: ', len(pca_train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: PCA Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create PCA object, train PCA and save the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to save pca\n",
    "import pickle\n",
    "\n",
    "def save_obj(obj, file_name):\n",
    "    with open(file_name + '.pkl', 'wb') as file:\n",
    "        pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(file_name):\n",
    "    with open(file_name + '.pkl', 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAProcess(pca_train: list):\n",
    "    # Create PCA object\n",
    "    pca = PCA(n_components = len(pca_train))\n",
    "\n",
    "    # Fit with training data\n",
    "    np_pca_train = np.array(pca_train)\n",
    "    pca.fit(np_pca_train)\n",
    "\n",
    "    # Save pca\n",
    "    save_obj(pca, 'pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCAProcess(pca_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Read testing data and perform rasterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadPCATestData(file_name):\n",
    "    test_data = []\n",
    "    with open(file = file_name, mode = 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            if not line:\n",
    "                break\n",
    "            tmp = line.split()\n",
    "            test_data.append([float(x) for x in tmp])\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def Standardize(points):\n",
    "    \n",
    "    return\n",
    "\n",
    "def Rasterize(\n",
    "    points, N: int,\n",
    "    x_min: float, x_max: float, \n",
    "    y_min: float, y_max: float, \n",
    "    z_min: float, z_max: float):\n",
    "\n",
    "    bool_arr = [0] * (N ** 3)    \n",
    "    for i in range(len(points)):\n",
    "        i_x = floor(N * (points[i][0] - x_min) / (x_max - x_min))\n",
    "        i_y = floor(N * (points[i][1] - y_min) / (y_max - y_min))\n",
    "        i_z = floor(N * (points[i][2] - z_min) / (z_max - z_min))\n",
    "        bool_arr[i_x + N * i_y + N * N * i_z] = 1\n",
    "    \n",
    "    return bool_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_test_data = ReadPCATestData('E:\\\\SRTP\\\\part\\\\000\\\\points.txt')\n",
    "pca_test_data = Rasterize(point_test_data, 32, -0.6, 0.6, -0.6, 0.6, -0.6, 0.6)\n",
    "pca_test_data = np.array([pca_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training data and testing data\n",
    "# # parameter ratio is the proportion of data for testing\n",
    "# def SplitPCAMatrixData(pca_matrix: list, ratio):\n",
    "#     size_for_train = int(len(pca_matrix) * (1 - ratio))\n",
    "#     return pca_matrix[:size_for_train], pca_matrix[size_for_train:]\n",
    "\n",
    "# pca_train, pca_test = SplitPCAMatrixData(pca_matrix, 0.05)\n",
    "\n",
    "# print('size of training data:', len(pca_train))\n",
    "# print('size of testing data:', len(pca_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Load `pca` and perform transformation on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = load_obj('pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train_result = pca.transform(pca_train_data)\n",
    "pca_test_result = pca.transform(pca_test_data)\n",
    "print(len(pca_train_result))\n",
    "print(len(pca_test_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Find the most similar model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a model in the testing set, and find the most similar model in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Iterate all the possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def FindMostSimilarModel1(train_result: list, test_result: list, index = 0):\n",
    "    min_dist = 999999999999\n",
    "    min_index = cur_index = 0\n",
    "    for row in train_result:\n",
    "        dist = np.linalg.norm(row - test_result[index])\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_index = cur_index\n",
    "        cur_index = cur_index + 1\n",
    "    return min_index\n",
    "\n",
    "def FindMostSimilarModel(train_result: list, test_result: list, index = 0, n = 1):\n",
    "    # When n is 1, it is more efficient to iterate directly rather than using priority queue\n",
    "    if n == 1:\n",
    "        return FindMostSimilarModel1(train_result, test_result, index)\n",
    "    \n",
    "    # When n is greater than 1, use priority queue\n",
    "    tuple_heap = []\n",
    "    cur_index = 0\n",
    "    for row in train_result:\n",
    "        cur_dist = np.linalg.norm(row - test_result[index])\n",
    "        cur_tuple = (cur_dist, cur_index)\n",
    "        heapq.heappush(tuple_heap, cur_tuple)\n",
    "        cur_index = cur_index + 1\n",
    "    return heapq.nsmallest(n, tuple_heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FindMostSimilarModel(pca_train_result, pca_test_result, n = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: HNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = len(pca_test_result[0])\n",
    "hnsw_index = hnswlib.Index('l2', dim = dimension)\n",
    "hnsw_index.init_index(max_elements = 157)\n",
    "hnsw_index.add_items(pca_train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the index object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(hnsw_index, 'hnsw_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the index object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_index = load_obj('hnsw_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_index.knn_query(pca_test_result, k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasketch import WeightedMinHashGenerator, MinHashLSH\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# lsh_train = np.array(result_train)\n",
    "# lsh_test = np.array(result_test)\n",
    "\n",
    "# mg = WeightedMinHashGenerator(lsh_train.shape[1])\n",
    "# lsh = MinHashLSH(threshold = 0.5)\n",
    "# for lsh_index, lsh_value in tqdm(enumerate(lsh_train)):\n",
    "#     m_hash = mg.minhash(lsh_value)\n",
    "#     lsh.insert(lsh_index, m_hash)\n",
    "\n",
    "# lsh_result = list()\n",
    "# for test_case in lsh_test:\n",
    "#     lsh_result.append(lsh.query(mg.minhash(test_case)))\n",
    "# print(lsh_result)\n",
    "\n",
    "# from datasketch import WeightedMinHashGenerator, MinHashLSHForest\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# lsh_train = np.array(result_train)\n",
    "# lsh_test = np.array(result_test)\n",
    "\n",
    "# mg = WeightedMinHashGenerator(lsh_train.shape[1])\n",
    "# forest = MinHashLSHForest()\n",
    "# for lsh_index, lsh_value in tqdm(enumerate(lsh_train)):\n",
    "#     m_hash = mg.minhash(lsh_value)\n",
    "#     forest.add(lsh_index, m_hash)\n",
    "\n",
    "# forest.index()\n",
    "# lsh_result = list()\n",
    "# for test_case in lsh_test:\n",
    "#     lsh_result.append(forest.query(mg.minhash(test_case), k = 6))\n",
    "# print(lsh_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c04e78781e6d9090880d595509d6719a2dd2d6bee35a08d3c7ffaa2ffefd9bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
